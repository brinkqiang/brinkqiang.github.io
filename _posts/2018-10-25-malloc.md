---
title: malloc
description:
categories:
 - malloc
tags:
---

# 1. 系统结构
系统下经典内存布局如上，程序起始的1GB地址为内核空间，接下来是向下增长的栈空间和向上增长的mmap地址。而堆地址是从底部开始，去除ELF、数据段、代码段、常量段之后的地址并向上增长。纵观各种内存布局，对于大内存各种malloc基本上都是直接mmap的。而对于小数据，则通过向操作系统申请扩大堆顶，这时候操作系统会把需要的内存分页映射过来，然后再由这些malloc管理这些堆内存块，减少系统调用。而在free内存的时候，不同的malloc有不同的策略，不一定会把内存真正地还给系统，所以很多时候，如果访问了free掉的内存，并不会立即Run Time Error，只有访问的地址没有对应的内存分页，才会崩掉。

## 1.1. Ptmalloc (glibc 的malloc实现)

### 1.1.1. 小内存分配与释放
在ptmalloc中采用chunk进行小内存管理，并且把相似（相同）大小的chunk组织在一个链表中进行维护，这个链表叫做bin。 前64个bin中组织的chunk大小按8个字节递增，这一块叫做 small_bin。 之后的就是large_bin, large_bin中的chunk是先按size，size相同则按照最近使用时间排列，这样要搜索一个可用的内存时，就在bins里按大小搜索，返回一个最小可用的chunk。

chunk结构如下图所示。可以理解成链表中一个node。存储了上一个相邻的chunk的大小以及flag，还有下一个chunk的“指针”。 flag A 表示是不是在主分配去，M表示是否是mmap得到的， P表示上一个chunk是否在使用中。

在free的时候，ptmalloc会检查附近的chunk，如果标志位为P，即空闲中，会尝试把连续空闲的chunk合并成一个大的chunk，放到unstored bin里。但是当很小的chunk释放的时候，ptmalloc会把它并入fast bin中。同样，某些时候，fast bin里的连续内存块会被合并并加入到一个unsorted bin里，然后再才进入普通bin里。所以malloc小内存的时候，是先查找fast bin，再查找unsorted bin，最后查找普通的bin，如果unsorted bin里的chunk不合适，则会把它扔到bin里。

### 1.1.2. 大内存分配与释放
Ptmalloc的分配的内存顶部还有一个top chunk，如果前面的bin里的空闲chunk都不足以满足需要，就是尝试从top chunk里分配内存。如果top chunk里也不够，就要从操作系统里拿了，这样会造成heap增大。
 
还有就是特别大的内存，会直接从系统mmap出来，不受chunk管理，这样的内存在回收的时候也会munmap还给操作系统。

### 1.1.3. 总结

#### 1.1.3.1. 分配与释放
- 小内存：（相当于实现了自身的cache，速度快）
分配：[获取分配区(arena)并加锁] -> fast bin -> unsorted bin -> small bin -> large bin -> top chunk -> 扩展堆 （brk分配）
释放：brk分配的内chunk list，只能从top开始线性向下释放。释放掉中间的chunk，无法归还给OS（内存孔洞），而是并链入到了bins/fast bins的容器中。

- 大内存： （与os直接打交道，速度慢）
分配：mmap从操作系统获得
释放：munmap归还给操作系统

#### 1.1.3.2. 人为控制：
大内存和小内存的界限： mallopt（） 函数可以调整threshold，大于此threshold的用mmap分配，小于此值的用brk缓存技术分配。 
 
mallopt(M_MMAP_MAX, 0); // 禁止malloc调用mmap分配内存  
mallopt(M_TRIM_THRESHOLD, 0); // 禁止内存缩进，sbrk申请的内存释放后不会归还给操作系统
 
如上的code相当于实现了一个内存池功能，这个会持续到线程结束才把内存归还给操作系统。 

M_MMAP_THRESHOLD 的设置是多大算大内存，多大算小内存的设置 
M_TRIM_THRESHOLD heap顶部攒到多大归还给操作系统，0则是不归还给操作系统

手动shrink heap： 使用malloc_trim（）根据man手册的解释，它应该是负责告诉glibc在brk维护的堆队列中，堆顶留下多少的空余空间（free space），其他往上的空余空间全部归还给系统。

手动设置每个线程的arena数量：mallopt(M_ARENA_MAX, 1) ，设置为0则为系统自动设置。

#### 1.1.3.3. 经验教训：

Ptmalloc默认后分配内存先释放，因为内存回收是从top chunk开始的。

避免多线程频繁分配和释放内存，会造成频繁加解锁。

不要分配长生命周期的内存块，容易造成内碎片，影响内存回收。

对于动态增长STL容器，要注意它维护的队列却是分配在heap上的。也就是说一个这样的临时对象所操作过的内存，依然可能产生碎片。如果这样的函数被频繁调用，碎片就会非常多。尽量成批reserve一块内存使用。减少在容器已满的情况下仍然push_back单个元素的操作，这样非常容易产生碎片。

即便我们做过shrink_to_fit的工作（std::vector<t*>(v).swap(v)），如果里面是碎片，那也会被驻留在brk维护的free_list中，不会被释放。

长时间的线上服务更应该注重编码习惯，尽量减少内存碎片。

每个线程至少有一个，至多有cores_num*8个自己的arena（看成by线程的内存池），减少锁的使用。不同arena之间不能交替使用。多尝试arena数目的设置，对虚拟内存的消耗有挺大的影响。

## 1.2. TCmalloc（google的实现）
http://goog-perftools.sourceforge.net/doc/tcmalloc.html

Tcmalloc是Google gperftools里的组件之一。全名是 thread cache malloc（线程缓存分配器）。tcmalloc正是通过thread cache这种机制实现了大多数情况下的无锁内存分配。优势如下：

速度快：号称比ptmalloc2.0速度快（tc 50纳秒情况下 pt需要300纳秒）。也号称github因为使用tcmalloc，性能提高了30%
减少锁争用：TCMalloc减少了多线程程序中的锁争用情况。对于小对象，几乎已经达到了零争用。对于大对象，TCMalloc尝试使用粒度较好和有效的自旋锁。
省内存：分配N个8字节对象使用大约8N * 1.01字节的空间。而ptmalloc2中每个对象都使用了一个四字节的头。
跨线程归还：尽管ptmalloc2也实现了per-thread缓存，第一个线程申请的内存，释放的时候还是必须放到第一个线程池中（不可移动），这样可能导致大量内存浪费。tcmalloc跨线程归还内存，是因为所有线程公用了底层的一个分配器，所以跨线程归还是无需加锁的。
tcmalloc与大多数现代分配器一样，使用的是基于页的内存分配，也就是说，这种内存分配的内部度量单位是页，而不是字节。这种内存分配可以有效地减少内存碎片，同时，也可以增加局部性。此外，也可以使得元数据的跟踪更为简单。tcmalloc定义一页为8K字节，在大多数的linux系统中，一页是4K字节，也就是tcmalloc的一页是linux的两页。
tcmalloc中的内存分配块整体来说分为两类，“小块”和“大块”，“小块”是小于kMaxPages的内存块，“小块”可以进一步分为size classes，而且“小块”的内存分配是通过thread cache或者central per-size class cache而实现。“大块”是大于等于kMaxPages的内存块，“大块”的内存分配是通过central PageHeap实现。

### 小内存分配
对于小块内存分配，其内部维护了60个不同大小的分配器（实际源码中看到的是86个），和ptmalloc不同的是，它的每个分配器的大小差是不同的，依此按8字节、16字节、32字节等间隔开。在内存分配的时候，会找到最小复合条件的，比如833字节到1024字节的内存分配请求都会分配一个1024大小的内存块。如果这些分配器的剩余内存不够了，会向中央堆申请一些内存，打碎以后填入对应分配器中。同样，如果中央堆也没内存了，就向中央内存分配器申请内存。

在线程缓存内的分配器中分别维护了一个大小固定的自由空间链表，直接由这些链表分配内存的时候是不加锁的。但是中央堆是所有线程共享的，在由其分配内存的时候会加自旋锁(spin lock)。

小内存分配的步骤如下：

把所需的小内存map到相应的size-class（如833则到1024在的class区操作）；
在这个线程的cache内相应的size-class中的free list寻找；
如果这个free list不为空（即有相应的可分配内存），删除list的头结点并返回这个头结点（整个过程在by 线程的cache中，不需要加锁，速度非常快，加锁解锁至少消耗100纳秒）
如果free list为空（这个size的缓存内存都被分配完了），则去central heap中的central free list中的相应size class中寻找
如果central free list 不为空， 把这块内存（申请了n个这个大小）放到thread local的free list, 并返回这个list的头结点，剩下n-1个在thread-local的free list中
如果central free list为空，向central page allocator申请m个内存页->按照所需要的size分割->放入central free list ->按照5放入 thread local 的free list
在线程内存池每次从中央堆申请内存的时候，分配多少内存也直接影响分配性能。申请地太少会导致频繁访问中央堆，也就会频繁加锁，而申请地太多会导致内存浪费。在tcmalloc里，这个每次申请的内存量是动态调整的，调整方式使用了类似把tcp窗口反过来用的慢启动（slow start）算法调整max_length， 每次申请内存是申请min(max_length,每个分配器对应的num_objects_to_move)个数的内存块。

 num_objects_to_move取值:以64K为基准，并且最小不低于2，最大不高于32的值。也就是说，对于大于等于32K的分配器这个值为2（好像没有这样的分配器 class），对于小于2K的分配器，统一为32。其他的会把数据调整到64K / size 的个数。

对于max_length取值：其更多是用于释放内存。max_length由1开始，在其小于num_objects_to_move的时候每次累加1，大于等于的num_objects_to_move 累加1。释放内存的时候，首先max_length会对齐到num_objects_to_move，然后在大于num_objects_to_move的释放次数超过一定阀值，则会按num_objects_to_move缩减大小。
 
### 大内存分配
    对于大内存分配(大于8个分页, 即32K)，tcmalloc直接在中央堆里central page free lists分配。中央堆的内存管理是以分页为单位的，同样按大小维护了256个空闲空间链表central list，前255个分别是1个分页、2个分页到255个分页的空闲空间，最后一个是更多分页的小的空间。这里的空间如果不够用，就会直接从系统申请了。

### 资源释放
TCMalloc管理的堆中有很多分页。连续的分页由一种叫span得对象来表示。span的状态有allocated和free两种。如果是free，这个span对象会被page heap linked-list 记录。如果是allocated，他要么是 作为大对象分配 ，或 作为小对象分配 （这时候span内记录了小对象的class size）。 page的编号可以用来判断每个page属于哪个span。如下图span a占有两个分页，b占有1个，c占有5个，d占有三个。


在32位系统中，span分为两级由中央分配器管理。第一级有2^5个节点，第二级是2^15个。32位总共只能有2^20个分页（每个分页4KB = 2^12）。64为中有三级管理。

1. 释放某个object

2. 找到该object所在的span

3. 如果是小对象，归入小对象分配器的空闲链表，如果空闲空间大于指定预置则归还给central free list; 如果是大对象，则会把物理地址连续的前后的span也找出来，如果空闲则合并，并归入central free list。

4 central list也会试图通过释放可用span列表的最后几个span来将不用的空间归还给OS
