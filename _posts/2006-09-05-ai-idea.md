---
title: ai-idea
description:
categories:
 - ai-idea
tags:
---

- 前言

学海无涯苦作舟

# 1. 目标

## 1.1. 阶段一。 通过导入面相 (声纹|指纹)数据 与 日元

自学习 面相 与 日元关系 推导出 计算公式


# 2. 想法

## 2.1. 内丹的训练过程， 即 深度神经网络 输入 训练 通过N层 网络 得到结果 中靶 脱靶

## 2.2. 修炼佛法， 即 输入信息 拟合  中靶 脱靶 佛法 即 兼容性更高的 可以解释一切的 拟合函数

# 3. 图形化测试

## 3.1. http://playground.tensorflow.org/

# 4. 流程

## 4.1. 简单神经元流程

### 4.1.1. 前向传播
想象一个工厂生产线。原材料(输入数据)从一端进入,经过多个工作站(神经元层),每个工作站都对材料进行一些加工(激活函数)。最后,成品(预测结果)从另一端出来。这就是前向传播的过程 - 信息从输入层流向输出层。
### 4.1.2. 代价函数 
把这比作是一个射箭比赛。你的目标是射中靶心(真实值),而你的箭落在靶上的某个位置(预测值)。代价函数就像是测量箭与靶心之间距离的尺子。距离越远,代价越大;距离越近,代价越小。你的目标是让这个"距离"尽可能小。
### 4.1.3. 反向传播
想象你在玩一个复杂的电子游戏,刚开始时你总是输。但每次失败后,游戏都会告诉你哪里做错了 - 可能是跳跃时机不对,或者选错了武器。你根据这些反馈调整策略,慢慢变得更擅长。反向传播就是这样,它从输出层开始,逐层回溯,找出每个"决策点"(权重和偏置)的贡献,告诉网络如何调整以减少错误。
### 4.1.4. 梯度下降
想象你在一个有很多山丘的黑暗地形中,你的目标是找到最低点。你可以感觉到脚下地面的倾斜程度(梯度)。你决定总是向着最陡峭的下坡方向走几步(学习率),然后再次检查周围的倾斜度。通过不断重复这个过程,你最终会到达山谷底部(最小值)。梯度下降就是这样工作的,它不断调整参数,试图找到使代价函数最小的点。

# 5. AI简单分类

## 5.1. 机器学习 系统

### 5.1.1. 监督学习 （任务驱动 ， 有历史数据）
#### 5.1.1.1. 分类
#### 5.1.1.2. 回归

### 5.1.2. 无监督学习 （数据驱动， 有历史数据）
#### 5.1.2.1. 分群 

### 5.1.3. 强化学习 （奖励机制， 无历史记录）

## 5.2. 深度学习 实现机制

# 6. AI 与 股票市场数据分析模型
